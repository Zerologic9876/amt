Practical No:- 11   
Aim:-  Map Reduce  
Software Used: VMware, Cent Os, Eclipse, Cloudera   
-----------

package wc;
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
package wc;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;

public class wcclass {

    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
        IntWritable one = new IntWritable(1);
        private final static Text word = new Text();

        public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter)
                throws IOException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                output.collect(word, one);
            }
        }
    }

    public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {
        public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output,
                Reporter reporter) throws IOException {
            int sum = 0;
            while (values.hasNext()) {
                sum += values.next().get();
            }
            output.collect(key, new IntWritable(sum));
        }
    }

    public static void main(String[] args) throws Exception {
        JobConf conf = new JobConf(wcclass.class);
        conf.setJobName("wordcount");
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(IntWritable.class);
        conf.setMapperClass(Map.class);
        conf.setCombinerClass(Reduce.class);
        conf.setReducerClass(Reduce.class);
        conf.setInputFormat(TextInputFormat.class);
        conf.setOutputFormat(TextOutputFormat.class);
        FileInputFormat.setInputPaths(conf, new Path(args[0]));
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));
        JobClient.runJob(conf);
    }
}
-----------

steps:- 
Fixed the class name to WordCount.
Moved the IntWritable one = new IntWritable(1); inside the Map class.
Corrected the Reduce class by specifying the correct types for the Reducer interface.
Updated the class name in the JobConf constructor to WordCount.class.
Make sure to replace your existing code with this corrected version.
-----------


Description:   
Hadoop Word Count is a classic and fundamental example in the world of big data and 
distributed computing. It demonstrates how to process and analyze a large collection of 
text documents to count the frequency of each unique word. This example is often used 
to introduce the Hadoop framework and MapReduce programming model.   
Procedure:   
On virtual box   
○ Computer > File System (/ ) > home > cloudera > downloads > (extract)   
○ Open Eclipse > create new java project   
In project hierarchy   
○ Src folder right click > build path > configure build path  ○ New opened 
window > Libraries >add external JARs..   
○ Open extracted folder > Select all JAR files   
○ Right click on project folder in hierarchy > New > Package   
○ Give package name > finish   
○ Right click on wc package > New > Class   
○ Give class name   
-----------------

Conclusion:-    
Hadoop has had a profound impact on the field of big data, enabling organizations to 
efficiently store, process, and analyze vast amounts of data. Its scalability, distributed 
architecture, and ecosystem of tools make it a crucial component in the big data 
landscape, helping organizations gain insights and make data-driven decisions. 
However, it's important to recognize that Hadoop is just one piece of the big data 
puzzle, and it is often used in conjunction with other technologies to meet specific data 
processing needs.   

--------
output:- 


